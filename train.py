# # File hu·∫•n luy·ªán
# topic = "giao_duc"

# # B1: N·∫°p d·ªØ li·ªáu t·ª´ file
# with open('giao_duc.txt', mode='r', encoding='utf-8') as f:
#     noi_dung = f.read()

# # B2: L√†m s·∫°ch (vi·∫øt th∆∞·ªùng, lo·∫°i d·∫•u c√¢u, kho·∫£ng tr·∫Øng)
# noi_dung = noi_dung.lower() # Vi·∫øt th∆∞·ªùng

# # C√°ch lo·∫°i d·∫•u c√¢u vi·∫øt d√†i
# # noi_dung = noi_dung.replace(',','').replace('.','').replace('-','').replace('?','').replace('!','')

# # C√°ch lo·∫°i d·∫•u c√¢u vi·∫øt ng·∫Øn - D√πng h√†m for
# # for dau_cau in [',','.','-','!','?',':']:
# #    noi_dung = noi_dung.replace(dau_cau,'')

# # C√°ch lo·∫°i d·∫•u c√¢u vi·∫øt ng·∫Øn - D√πng regax
# import re
# noi_dung = re.sub(r'[,.?!@#]', '', noi_dung)


# # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng
# noi_dung = noi_dung.replace(' ', ' ')

# # B3: Lo·∫°i b·ªè c√°c t·ª´ kh√¥ng lquan(stop words)
# with open('/Users/hungduc/Desktop/th∆∞ m·ª•c kh√¥ng c√≥ ti√™u ƒë·ªÅ 5/chua bai assm/stopwords.txt', mode='r', encoding='utf-8') as f:

#     # stop_words = f.readlines() # C√°ch n√†y c√≤n d·∫•u '\n' ·ªü cu·ªëi c√¢u
#     stop_words = [line.rstrip() for line in f] # C√°ch n√†y kh c√≤n d·∫•u '\n'
# for word in stop_words:
#     noi_dung = noi_dung.replace(word,'')

# # B4: T√°ch t·ª´
# ds_tu = noi_dung.split(' ')

# # B5: ƒê·∫øm s·ªë l·∫ßn xu√¢ts hi·ªán c·ªßa c√°c t·ª´
# tan_suat = {}
# for tu in ds_tu:
#     if tu not in tan_suat:
#         tan_suat[tu] = 1
#     else:
#         tan_suat[tu] += 1

# # B6: L∆∞u kqua v√†o filefile
# file_out = topic + '-f.txt'
# with open(file_out, mode='w',encoding='utf-8') as f:
#     for tu in tan_suat:
#         f.write(f'{tu} : {tan_suat[tu]}\n')

# file: tan_suat_tu_streamlit.py
import streamlit as st
import re
import requests

st.title("Ph√¢n t√≠ch t·∫ßn su·∫•t t·ª´ ƒë∆°n gi·∫£n")

# üìå T·∫£i stopwords t·ª´ Google Sheets (d·∫°ng CSV)
@st.cache_data
def lay_stopwords_tu_google_sheets():
    url = "https://docs.google.com/spreadsheets/d/e/2PACX-1vR5S-EGtFGIN8nKcGFjFaeP3BBCPCjsqv4s5iQUJJKumdaZB31Z4R0gUDLVUOWIPElNRUR64HJ40wXf/pub?output=csv"
    response = requests.get(url)
    response.encoding = "utf-8"
    dong = response.text.splitlines()
    return [tu.strip() for tu in dong if tu.strip()]

stopwords = lay_stopwords_tu_google_sheets()

# B1. T·∫£i t·ªáp vƒÉn b·∫£n
van_ban_file = st.file_uploader("t·∫£i l√™n t·ªáp vƒÉn b·∫£n txt", type="txt")

if van_ban_file:
    # B2. ƒê·ªçc v√† l√†m s·∫°ch vƒÉn b·∫£n
    text = van_ban_file.read().decode("utf-8").lower()
    text = re.sub(r"[,.?!@#]", "", text)

    # B3. Lo·∫°i stopword
    for sw in stopwords:
        text = text.replace(f" {sw} ", " ")

    # B4. T√°ch t·ª´ v√† ƒë·∫øm
    freq = {}
    for word in text.split():
        freq[word] = freq.get(word, 0) + 1

    # Hi·ªÉn th·ªã b·∫£ng k·∫øt qu·∫£
    st.write("K·∫øt qu·∫£ t·∫ßn su·∫•t:")
    for w, c in sorted(freq.items(), key=lambda x: x[1], reverse=True):
        st.write(f"{w} : {c}")

    # Cho ph√©p t·∫£i v·ªÅ
    ket_qua = "\n".join(f"{w} : {c}" for w, c in freq.items())
    st.download_button("T·∫£i file k·∫øt qu·∫£", ket_qua,
                       file_name="ket_qua.txt", mime="text/plain")
else:
    st.info(" ")
